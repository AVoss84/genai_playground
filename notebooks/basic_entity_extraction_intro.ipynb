{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This little tutorial focuses on the task of entity extraction from a PDF file using OCR + GenAI. The use of LangChain is illustrated as a convenient tool library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from my_package.config import global_config as glob\n",
    "# from my_package.config import config as cfg\n",
    "from my_package.utils.utils import s3_health_check \n",
    "\n",
    "#cfg.model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_health_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Work with PDF locally by downloading it from S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The S3 bucket and file name of the PDF\n",
    "bucket_name = 'sagemaker-foundry-610968375774-eu-central-1'\n",
    "object_key = 'firmware-migration-data/raw_documents_from_foundry/'\n",
    "local_filename = '005fed92-d96d-11eb-bd22-030439b8049b.pdf'\n",
    "\n",
    "# Initialize a boto3 client\n",
    "s3 = boto3.client('s3')\n",
    "    \n",
    "# Download the PDF file from S3\n",
    "s3.download_file(Bucket=bucket_name, Key=os.path.join(object_key, local_filename), Filename=os.path.join(glob.UC_DATA_PKG_DIR, local_filename))\n",
    "print(\"Download successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using convenience wrapper class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_package.services import s3_client\n",
    "\n",
    "my_client = s3_client.S3Client()\n",
    "\n",
    "my_client.download_file(key=os.path.join(object_key, local_filename), file_path=os.path.join(glob.UC_DATA_PKG_DIR, local_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) Use Textract to retrieve raw text from single PDF (stored in S3)\n",
    "\n",
    "Check out the textractor package! pip install amazon-textract-textractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a boto3 client for Textract\n",
    "textract = boto3.client('textract')\n",
    "\n",
    "# Call Amazon Textract\n",
    "response = textract.start_document_text_detection(\n",
    "    DocumentLocation={\n",
    "        'S3Object': {\n",
    "            'Bucket': bucket_name,\n",
    "            'Name': os.path.join(object_key, local_filename)\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# The job ID is needed to get the result\n",
    "job_id = response['JobId']\n",
    "print(f\"Started job with id: {job_id}\")\n",
    "\n",
    "# Check the job status and wait for it to complete\n",
    "status = ''\n",
    "time.sleep(5)  # Wait a few seconds before checking the status\n",
    "while status != 'SUCCEEDED':\n",
    "    result = textract.get_document_text_detection(JobId=job_id)\n",
    "    status = result['JobStatus']\n",
    "    if status == 'FAILED':\n",
    "        raise Exception(\"Textract Job Failed\")\n",
    "    time.sleep(5)  # Wait between checks\n",
    "\n",
    "# Collect lines of raw text in a list\n",
    "lines = []\n",
    "for block in result['Blocks']:\n",
    "    if block['BlockType'] == 'LINE':\n",
    "        #print(block['Text'])\n",
    "        lines.append(block['Text'])\n",
    "\n",
    "df = pd.DataFrame(lines, columns=['Extracted Text:']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.head(50))\n",
    "#df.to_csv('extracted_text.csv', index=False)\n",
    "\n",
    "# Concatenate all the text into a single string (only for keeping things simple here)\n",
    "document = df['Extracted Text:'].str.cat(sep=' ')\n",
    "\n",
    "len(document)                  # just checkin for context window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textractor import Textractor\n",
    "from textractor.data.constants import TextractFeatures\n",
    "\n",
    "extractor = Textractor(profile_name=\"default\")\n",
    "\n",
    "filename = \"ZASO_Deklaration_SACH_BU_01.01.2024_19.pdf\"\n",
    "\n",
    "# # Synchronous analysis - single page\n",
    "# document = extractor.analyze_document(\n",
    "#     file_source = os.path.join(glob.UC_DATA_PKG_DIR, filename),      # local file path\n",
    "#     features=[TextractFeatures.FORMS, TextractFeatures.TABLES]\n",
    "# )\n",
    "\n",
    "# Define your S3 bucket and file path\n",
    "s3_bucket = \"midcorp-migration\"\n",
    "s3_key = os.path.join(\"midcorp-migration/sample_data/pdfs\", filename)\n",
    "\n",
    "# Asynchronous analysis for multi-page documents\n",
    "# Start asynchronous analysis\n",
    "my_doc = extractor.start_document_analysis(\n",
    "    file_source = os.path.join(glob.UC_DATA_PKG_DIR, filename),  # local file path\n",
    "    features=[TextractFeatures.FORMS, TextractFeatures.TABLES],\n",
    "    s3_upload_path=f\"s3://{s3_bucket}/{s3_key}\"\n",
    ")\n",
    "\n",
    "raw_text = my_doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Extract some basic entities from the Textract results using Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custome Callback handler:\n",
    "https://python.langchain.com/v0.1/docs/modules/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from uuid import UUID\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "# from langchain_core.callbacks import StdOutCallbackHandler\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "# Implement a callback handler that logs the generated text and the number of tokens used\n",
    "class BedrockHandler(BaseCallbackHandler):\n",
    "\n",
    "    def __init__(self, initial_text=\"\"):\n",
    "        self.text = initial_text\n",
    "        self.input_token_count = 0\n",
    "        self.output_token_count = 0\n",
    "        self.stop_reason = None\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        self.text += token\n",
    "        # do something\n",
    "\n",
    "    def on_llm_end(\n",
    "        self,\n",
    "        response: LLMResult,\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        parent_run_id: UUID | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Any:\n",
    "        if response.llm_output is not None:\n",
    "            self.input_token_count = response.llm_output.get(\"usage\", {}).get(\"prompt_tokens\", None)\n",
    "            self.output_token_count = response.llm_output.get(\"usage\", {}).get(\"completion_tokens\", None)\n",
    "            self.stop_reason = response.llm_output.get(\"stop_reason\", None)\n",
    "\n",
    "\n",
    "# class MyCustomHandler(BaseCallbackHandler):\n",
    "#     def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "#         print(f\"My custom handler, token: {token}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output format: JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "class OutputStructure(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the output structure for customer information.\n",
    "\n",
    "    Attributes:\n",
    "        first_name (str): First name of the customer.\n",
    "        last_name (str): Last name of the customer.\n",
    "        email (str): Email address of the customer.\n",
    "    \"\"\"\n",
    "    first_name: str = Field(description=\"First name of the customer\")\n",
    "    last_name: str = Field(description=\"Last name of the customer\")\n",
    "    email: str = Field(description=\"Email address of the customer\")\n",
    " \n",
    "parser = JsonOutputParser(pydantic_object=OutputStructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "from pprint import PrettyPrinter\n",
    " \n",
    "# handler1 = StdOutCallbackHandler()     # verbose output\n",
    "handler2 = BedrockHandler()\n",
    "# handler3 = MyCustomHandler()\n",
    "\n",
    "inference_modifier = {\n",
    "    \"max_tokens\": 2000,\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "model = ChatBedrock(\n",
    "        #model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        client=bedrock_runtime,\n",
    "        model_kwargs=inference_modifier\n",
    "    )\n",
    "  \n",
    "template = \"\"\"\n",
    "Following is an insurance contract. Please extract the following information from it: \n",
    "First name of the customer, the last name of the customer and its Email address: {context}.\\n \n",
    "Please create the following json output format:\\n{format_instructions}\n",
    "\"\"\"\n",
    " \n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    " \n",
    "chain = prompt | model | parser\n",
    "# chain = LLMChain(llm=model, prompt=prompt, callbacks=[handler2], verbose=False, output_parser=parser)  # alternative way to define chain\n",
    "  \n",
    "# response = chain.invoke({\"context\": document})\n",
    "response = chain.invoke({\"context\": document}, RunnableConfig(callbacks=[handler2]))\n",
    " \n",
    "PrettyPrinter().pprint(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_token_count = response.response_metadata.get(\"usage\", {}).get(\"prompt_tokens\", 0)\n",
    "# output_token_count = response.response_metadata.get(\"usage\", {}).get(\"completion_tokens\", 0)   # via boto3 response\n",
    "\n",
    "print(\"Input token count:\", handler2.input_token_count)\n",
    "print(\"Output token count:\", handler2.output_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_firmware",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
